{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the output confirms that DeepFace returns a list with a dictionary for each detected face. Here's how you can update your code to handle the returned data correctly and display the dominant emotion on the overlay:\n",
    "\n",
    "Updated Code for DeepFace Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Load OpenCV's pre-trained Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y + h, x:x + w]  # Extract the face region\n",
    "\n",
    "        try:\n",
    "            # Ensure the face ROI is valid before analysis\n",
    "            if face_roi.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Analyze emotions for the detected face\n",
    "            analysis = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "            if isinstance(analysis, list):  # If multiple faces, process the first one\n",
    "                analysis = analysis[0]\n",
    "\n",
    "            # Extract the dominant emotion correctly\n",
    "            dominant_emotion = analysis['dominant_emotion']\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Add emotion label\n",
    "            cv2.putText(frame, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' key or if window is closed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Emotion Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, **DeepFace** can handle face detection internally, so you don't necessarily need to use an external face detection model like OpenCV's Haar cascade. When you call `DeepFace.analyze()`, it will automatically detect faces if none are provided. This is very convenient since it simplifies the process by eliminating the need for manual face detection.\n",
    "\n",
    "Here's how you can modify your code to rely purely on **DeepFace** for both face detection and emotion analysis:\n",
    "\n",
    "### **Updated Code Using DeepFace for Face Detection and Emotion Analysis**:\n",
    "\n",
    "\n",
    "\n",
    "### **Changes in the Code**:\n",
    "\n",
    "1. **Automatic Face Detection**:\n",
    "   - The `DeepFace.analyze()` function automatically detects faces in the provided image (in this case, from your webcam feed). You no longer need to manually use OpenCV's Haar Cascade for face detection.\n",
    "\n",
    "2. **Emotion Analysis**:\n",
    "   - The `actions=['emotion']` argument instructs DeepFace to perform emotion detection on the detected faces.\n",
    "\n",
    "3. **Handling Multiple Faces**:\n",
    "   - DeepFace will return a list of analyses for each face detected. The loop processes each face and draws a bounding box and emotion label.\n",
    "\n",
    "4. **`enforce_detection=False`**:\n",
    "   - This argument allows DeepFace to process the image even if no face is detected. If no faces are found, the code won't raise an error.\n",
    "\n",
    "### **How DeepFace Handles Face Detection**:\n",
    "- **DeepFace** uses a variety of face detection models (like `opencv`, `mtcnn`, `dlib`, or `retinaface`) internally depending on the configuration or availability. By default, DeepFace automatically chooses the appropriate model based on the environment and the requirements.\n",
    "\n",
    "### **Running the Code**:\n",
    "When you run this code, DeepFace will handle everything:\n",
    "- **Face detection** (no need for `cv2.CascadeClassifier`)\n",
    "- **Emotion detection** for each face detected\n",
    "- Display the webcam feed with bounding boxes and emotion labels in real time.\n",
    "\n",
    "This version is simpler and avoids the need to integrate external face detection models manually. Let me know if you encounter any issues or need further modifications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error you're encountering — **\"too many values to unpack (expected 4)\"** — occurs when trying to unpack the face detection results incorrectly. In the DeepFace output, the face region is returned as a dictionary, and you're trying to unpack it as if it were a tuple (which it isn't).\n",
    "\n",
    "This issue happens when you try to access the face region in an incorrect format.\n",
    "\n",
    "### **Solution**:\n",
    "In DeepFace's output, the **face region** is a dictionary with the keys `x`, `y`, `w`, and `h` representing the coordinates and size of the detected face. Instead of trying to unpack it directly into four variables, you should access the values using their respective keys.\n",
    "\n",
    "Here's the corrected code:\n",
    "\n",
    "### **Corrected Code**:\n",
    "\n",
    "\n",
    "### **Key Changes**:\n",
    "1. **Accessing the face region correctly**: \n",
    "   - The face region is stored as a dictionary with keys `'x'`, `'y'`, `'w'`, and `'h'`. \n",
    "   - Instead of unpacking it like a tuple, we access the values directly using `face_region['x']`, `face_region['y']`, `face_region['w']`, and `face_region['h']`.\n",
    "\n",
    "This should fix the \"too many values to unpack\" error you're seeing. \n",
    "\n",
    "### **Explanation**:\n",
    "- **DeepFace Output Structure**: The result of `DeepFace.analyze()` contains a list of dictionaries (one per detected face). Each dictionary contains keys like `'dominant_emotion'`, `'region'`, and other face analysis information.\n",
    "- **Face Region**: The `'region'` key contains another dictionary with the face's bounding box coordinates, and we access those coordinates individually instead of trying to unpack them all at once.\n",
    "\n",
    "Let me know if this resolves the issue or if you encounter any other problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Analyze the image with DeepFace (detect faces and emotions internally)\n",
    "        analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        \n",
    "        # Loop through all detected faces (if multiple faces are detected)\n",
    "        for face_analysis in analysis:\n",
    "            # Get the dominant emotion\n",
    "            dominant_emotion = face_analysis['dominant_emotion']\n",
    "\n",
    "            # Get the region of the detected face (as a dictionary, not a tuple)\n",
    "            face_region = face_analysis['region']\n",
    "            x, y, w, h = face_region['x'], face_region['y'], face_region['w'], face_region['h']\n",
    "\n",
    "            # Draw bounding box around the face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Add the emotion label\n",
    "            cv2.putText(frame, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "    # Display the video feed with detected emotions\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' key or if the window is closed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Emotion Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
