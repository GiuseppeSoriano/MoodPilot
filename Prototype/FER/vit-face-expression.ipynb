{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Vision Transformer (ViT) for Facial Expression Recognition Model Card\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "- **Model Name:** [trpakov/vit-face-expression](https://huggingface.co/trpakov/vit-face-expression)\n",
    "\n",
    "- **Task:** Facial Expression/Emotion Recognition\n",
    "\n",
    "- **Dataset:** [FER2013](https://www.kaggle.com/datasets/msambare/fer2013)\n",
    "\n",
    "- **Model Architecture:** [Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)\n",
    "\n",
    "- **Finetuned from model:** [vit-base-patch16-224-in21k](https://huggingface.co/google/vit-base-patch16-224-in21k)\n",
    "\n",
    "## Model Description\n",
    "\n",
    "The vit-face-expression model is a Vision Transformer fine-tuned for the task of facial emotion recognition. \n",
    "\n",
    "It is trained on the FER2013 dataset, which consists of facial images **categorized** into seven different emotions:\n",
    "- Angry\n",
    "- Disgust\n",
    "- Fear\n",
    "- Happy\n",
    "- Sad\n",
    "- Surprise\n",
    "- Neutral\n",
    "\n",
    "(Sì il FER2013 ha solamente 7 emozioni e non 8)\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "- **Validation set accuracy:** 0.7113\n",
    "- **Test set accuracy:** 0.7116\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how you can integrate the **\"trpakov/vit-face-expression\"** model with OpenCV to perform real-time emotion detection using your webcam feed. The process involves loading the pre-trained model and image processor from Hugging Face, detecting faces in the video stream using OpenCV, and analyzing the emotions using the model.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation**:\n",
    "1. **Model and Processor**:\n",
    "   - The model (`AutoModelForImageClassification`) and processor (`AutoImageProcessor`) are loaded from Hugging Face. \n",
    "   - The processor handles image preprocessing like resizing, normalization, and tensor conversion.\n",
    "   \n",
    "2. **Face Detection**:\n",
    "   - OpenCV's Haar cascade is used to detect faces in the video frame.\n",
    "   - Each detected face is cropped from the frame (`face_roi`) for emotion classification.\n",
    "\n",
    "3. **Emotion Analysis**:\n",
    "   - The face is converted into a PIL image and passed through the processor for preprocessing.\n",
    "   - The preprocessed image is fed into the model to get the logits.\n",
    "   - Softmax is applied to convert logits to probabilities, and the class with the highest probability is selected as the predicted emotion.\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Bounding boxes are drawn around the detected faces.\n",
    "   - The predicted emotion is displayed as a label near the bounding box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "# Load OpenCV's Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face ROI\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        try:\n",
    "            # Convert to PIL Image and preprocess\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))\n",
    "            inputs = processor(face_pil, return_tensors=\"pt\")\n",
    "\n",
    "            # Run inference\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = probs.argmax().item()\n",
    "            predicted_label = model.config.id2label[predicted_class]\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' key or if the window is closed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Emotion Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it's absolutely possible to display the current **frames per second (FPS)** on the preview window. To calculate FPS in OpenCV, you can use a simple approach by measuring the time taken to process each frame. By keeping track of the time before and after processing each frame, you can calculate the FPS and display it on the video stream.\n",
    "\n",
    "\n",
    "### **Changes for FPS Calculation**:\n",
    "1. **Track Time for FPS**:\n",
    "   - We use `time.time()` to get the current time before and after each frame is processed.\n",
    "   - The FPS is calculated as the inverse of the difference between the current time and the previous time (`1 / (curr_time - prev_time)`).\n",
    "\n",
    "2. **Display FPS**:\n",
    "   - We use `cv2.putText()` to draw the FPS value on the frame. The text is placed in the top-left corner of the window (`(10, 30)`).\n",
    "\n",
    "3. **Frame Processing**:\n",
    "   - For each frame, we calculate the FPS and update the `prev_time` for the next frame.\n",
    "\n",
    "\n",
    "- FPS is calculated based on the time elapsed between frames. The faster the frames are processed, the higher the FPS will be. This gives you a real-time indication of how many frames are processed per second.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "# Load OpenCV's Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# FPS calculation variables\n",
    "prev_time = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get the current time\n",
    "    curr_time = time.time()\n",
    "\n",
    "    # Calculate FPS (frames per second)\n",
    "    fps = 1 / (curr_time - prev_time)\n",
    "    prev_time = curr_time\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face ROI\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        try:\n",
    "            # Convert to PIL Image and preprocess\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))\n",
    "            inputs = processor(face_pil, return_tensors=\"pt\")\n",
    "\n",
    "            # Run inference\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = probs.argmax().item()\n",
    "            predicted_label = model.config.id2label[predicted_class]\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the video feed with detected emotions\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' key or if the window is closed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Emotion Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
