{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vision Transformer (ViT) for Facial Expression Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Explanation**:\n",
    "1. **Model and Processor**:\n",
    "   - The model (`AutoModelForImageClassification`) and processor (`AutoImageProcessor`) are loaded from Hugging Face. \n",
    "   - The processor handles image preprocessing like resizing, normalization, and tensor conversion.\n",
    "   \n",
    "2. **Face Detection**:\n",
    "   - OpenCV's Haar cascade is used to detect faces in the video frame.\n",
    "   - Each detected face is cropped from the frame (`face_roi`) for emotion classification.\n",
    "\n",
    "3. **Emotion Analysis**:\n",
    "   - The face is converted into a PIL image and passed through the processor for preprocessing.\n",
    "   - The preprocessed image is fed into the model to get the logits.\n",
    "   - Softmax is applied to convert logits to probabilities, and the class with the highest probability is selected as the predicted emotion.\n",
    "\n",
    "4. **Visualization**:\n",
    "   - Bounding boxes are drawn around the detected faces.\n",
    "   - The predicted emotion is displayed as a label near the bounding box.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "# Load OpenCV's Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face ROI\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        try:\n",
    "            # Convert to PIL Image and preprocess\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))\n",
    "            inputs = processor(face_pil, return_tensors=\"pt\")\n",
    "\n",
    "            # Run inference\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = probs.argmax().item()\n",
    "            predicted_label = model.config.id2label[predicted_class]\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' key or if the window is closed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Emotion Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the current **frames per second (FPS)** on the preview window: to calculate FPS in OpenCV, you can use a simple approach by measuring the time taken to process each frame. By keeping track of the time before and after processing each frame, you can calculate the FPS and display it on the video stream.\n",
    "\n",
    "\n",
    "### **Changes for FPS Calculation**:\n",
    "1. **Track Time for FPS**:\n",
    "   - We use `time.time()` to get the current time before and after each frame is processed.\n",
    "   - The FPS is calculated as the inverse of the difference between the current time and the previous time (`1 / (curr_time - prev_time)`).\n",
    "\n",
    "2. **Display FPS**:\n",
    "   - We use `cv2.putText()` to draw the FPS value on the frame. The text is placed in the top-left corner of the window (`(10, 30)`).\n",
    "\n",
    "3. **Frame Processing**:\n",
    "   - For each frame, we calculate the FPS and update the `prev_time` for the next frame.\n",
    "\n",
    "\n",
    "- FPS is calculated based on the time elapsed between frames. The faster the frames are processed, the higher the FPS will be. This gives you a real-time indication of how many frames are processed per second.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "# Load OpenCV's Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# FPS calculation variables\n",
    "prev_time = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get the current time\n",
    "    curr_time = time.time()\n",
    "\n",
    "    # Calculate FPS (frames per second)\n",
    "    fps = 1 / (curr_time - prev_time)\n",
    "    prev_time = curr_time\n",
    "\n",
    "    # Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face ROI\n",
    "        face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "        try:\n",
    "            # Convert to PIL Image and preprocess\n",
    "            face_pil = Image.fromarray(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))\n",
    "            inputs = processor(face_pil, return_tensors=\"pt\")\n",
    "\n",
    "            # Run inference\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            predicted_class = probs.argmax().item()\n",
    "            predicted_label = model.config.id2label[predicted_class]\n",
    "\n",
    "            # Draw bounding box and label\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "    # Display FPS on the frame\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the video feed with detected emotions\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' key or if the window is closed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Emotion Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final version of the code. For additional details, refer to `Prototype/FER/Models/Info.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 963 frames in 189.03 seconds. FPS: 5.09\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "# Load model and processor\n",
    "processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "# Load OpenCV's Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Define a buffer for recent emotion scores\n",
    "maxlen = 15\n",
    "recent_scores = deque(maxlen=maxlen)\n",
    "\n",
    "def get_log_filename(base_name=\"logs/emotion_log\"):\n",
    "    \"\"\"Generate a unique log filename by appending a counter if files already exist.\"\"\"\n",
    "    os.makedirs(\"logs\", exist_ok=True)  # Ensure the logs folder exists\n",
    "    counter = 0\n",
    "    while True:\n",
    "        filename = f\"{base_name}_{counter}.txt\"\n",
    "        if not os.path.exists(filename):\n",
    "            return filename\n",
    "        counter += 1\n",
    "\n",
    "def process_video(video_file, show_preview=True):\n",
    "    # Determine video source\n",
    "    video_source = 0 if video_file == \"camera\" else video_file\n",
    "\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source '{video_source}'.\")\n",
    "        return\n",
    "\n",
    "    # Generate a unique log filename\n",
    "    log_filename = get_log_filename()\n",
    "    log_file = open(log_filename, \"w\")\n",
    "    log_file.write(\"Timestamp,Emotion,Score\\n\")\n",
    "\n",
    "    # Variables for FPS calculation\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Extract face ROI\n",
    "            face_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "            try:\n",
    "                # Convert to PIL Image and preprocess\n",
    "                face_pil = Image.fromarray(cv2.cvtColor(face_roi, cv2.COLOR_BGR2RGB))\n",
    "                inputs = processor(face_pil, return_tensors=\"pt\")\n",
    "\n",
    "                # Run inference\n",
    "                outputs = model(**inputs)\n",
    "                probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "                recent_scores.append(probs.detach().numpy())\n",
    "\n",
    "                # Compute smoothed scores\n",
    "                avg_scores = np.mean(recent_scores, axis=0).squeeze()\n",
    "                predicted_class = np.argmax(avg_scores)\n",
    "                predicted_label = model.config.id2label[predicted_class]\n",
    "\n",
    "                if video_file == \"camera\":\n",
    "                    # Get the current timestamp\n",
    "                    timestamp = time.time()\n",
    "                else:\n",
    "                    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0  # Convert to seconds\n",
    "\n",
    "                # Log emotion and score\n",
    "                log_file.write(f\"{timestamp:.2f},{predicted_label},{avg_scores.tolist()}\\n\")\n",
    "\n",
    "                # Display the detected emotion on the video (if preview is enabled)\n",
    "                if show_preview:\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, predicted_label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Show the video frame with emotion labels (if preview is enabled)\n",
    "        if show_preview:\n",
    "            cv2.imshow(\"ViT Expression Recognition\", frame)\n",
    "\n",
    "            # Break on 'q' key press or window close\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            if cv2.getWindowProperty(\"ViT Expression Recognition\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Processed {frame_count} frames in {total_time:.2f} seconds. FPS: {fps:.2f}\")\n",
    "\n",
    "    # Log the FPS\n",
    "    log_file.write(f\"\\nProcessed {frame_count} frames in {total_time:.2f} seconds.\\n\")\n",
    "    log_file.write(f\"FPS: {fps:.2f}\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if show_preview:\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Replace 'input_video.mp4' with the path to your video file or use \"camera\" for live feed\n",
    "    process_video('../ExampleVideos/cicciogamer89.mp4', show_preview=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
