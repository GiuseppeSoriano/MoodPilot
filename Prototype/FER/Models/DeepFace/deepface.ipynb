{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DeepFace** can handle face detection internally, so you don't necessarily need to use an external face detection model like OpenCV's Haar cascade. When you call `DeepFace.analyze()`, it will automatically detect faces if none are provided. This is very convenient since it simplifies the process by eliminating the need for manual face detection. **DeepFace** uses a variety of face detection models (like `opencv`, `mtcnn`, `dlib`, or `retinaface`) internally depending on the configuration or availability. By default, DeepFace automatically chooses the appropriate model based on the environment and the requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Analyze the image with DeepFace (detect faces and emotions internally)\n",
    "        #\n",
    "        # Set enforce_detection=False: \n",
    "        #   This argument allows DeepFace to process the image even if no faces are detected.\n",
    "        #   If no faces are found, the code won't raise an error.\n",
    "        analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "        \n",
    "        # Loop through all detected faces (if multiple faces are detected)\n",
    "        for face_analysis in analysis:\n",
    "            # Get the dominant emotion\n",
    "            dominant_emotion = face_analysis['dominant_emotion']\n",
    "\n",
    "            # Get the region of the detected face (as a dictionary, not a tuple)\n",
    "            face_region = face_analysis['region']\n",
    "            x, y, w, h = face_region['x'], face_region['y'], face_region['w'], face_region['h']\n",
    "\n",
    "            # Draw bounding box around the face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Add the emotion label\n",
    "            cv2.putText(frame, dominant_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing face: {e}\")\n",
    "\n",
    "    # Display the video feed with detected emotions\n",
    "    cv2.imshow(\"Emotion Detection\", frame)\n",
    "\n",
    "    # Exit on 'q' key or if the window is closed\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q') or cv2.getWindowProperty(\"Emotion Detection\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final version of the code. For additional details, refer to `Prototype/FER/Models/Info.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 523 frames in 62.49 seconds. FPS: 8.37\n"
     ]
    }
   ],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "# Define a buffer for recent emotion scores\n",
    "maxlen = 15\n",
    "recent_scores = deque(maxlen=maxlen)\n",
    "\n",
    "def get_log_filename(base_name=\"logs/emotion_log\"):\n",
    "    \"\"\"Generate a unique log filename by appending a counter if files already exist.\"\"\"\n",
    "    os.makedirs(\"logs\", exist_ok=True)  # Ensure the logs folder exists\n",
    "    counter = 0\n",
    "    while True:\n",
    "        filename = f\"{base_name}_{counter}.txt\"\n",
    "        if not os.path.exists(filename):\n",
    "            return filename\n",
    "        counter += 1\n",
    "\n",
    "def process_video(video_file, show_preview=True):\n",
    "    # Determine video source\n",
    "    video_source = 0 if video_file == \"camera\" else video_file\n",
    "\n",
    "    # Open video capture\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source '{video_source}'.\")\n",
    "        return\n",
    "\n",
    "    # Generate a unique log filename\n",
    "    log_filename = get_log_filename()\n",
    "    log_file = open(log_filename, \"w\")\n",
    "    log_file.write(\"Timestamp,Emotion,Confidence\\n\")\n",
    "\n",
    "    # Variables for FPS calculation\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            # Analyze the image with DeepFace (detect faces and emotions internally)\n",
    "            analysis = DeepFace.analyze(frame, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "            for face_analysis in analysis:\n",
    "                # Get the dominant emotion and its confidence score\n",
    "                dominant_emotion = face_analysis['dominant_emotion']\n",
    "                emotion_scores = face_analysis['emotion']\n",
    "\n",
    "                # Smooth the scores using a deque\n",
    "                recent_scores.append(list(emotion_scores.values()))\n",
    "                smoothed_scores = np.mean(recent_scores, axis=0)\n",
    "                smoothed_emotion_idx = np.argmax(smoothed_scores)\n",
    "                smoothed_emotion = list(emotion_scores.keys())[smoothed_emotion_idx]\n",
    "\n",
    "                # Get the region of the detected face\n",
    "                face_region = face_analysis['region']\n",
    "                x, y, w, h = face_region['x'], face_region['y'], face_region['w'], face_region['h']\n",
    "\n",
    "                if video_file == \"camera\":\n",
    "                    # Get the current timestamp\n",
    "                    timestamp = time.time()\n",
    "                else:\n",
    "                    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0  # Convert to seconds\n",
    "\n",
    "                # Log the emotion and confidence\n",
    "                log_file.write(f\"{timestamp:.2f},{smoothed_emotion},{smoothed_scores[smoothed_emotion_idx]:.2f}\\n\")\n",
    "\n",
    "                # Draw bounding box and emotion label (if preview is enabled)\n",
    "                if show_preview:\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, smoothed_emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing frame: {e}\")\n",
    "\n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "        # Show the video feed (if preview is enabled)\n",
    "        if show_preview:\n",
    "            cv2.imshow(\"DeepFace Expression Recognition\", frame)\n",
    "\n",
    "            # Break on 'q' key press or window close\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            if cv2.getWindowProperty(\"DeepFace Expression Recognition\", cv2.WND_PROP_VISIBLE) < 1:\n",
    "                break\n",
    "\n",
    "    # Calculate FPS\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    fps = frame_count / total_time\n",
    "    print(f\"Processed {frame_count} frames in {total_time:.2f} seconds. FPS: {fps:.2f}\")\n",
    "\n",
    "    # Log the FPS\n",
    "    log_file.write(f\"\\nProcessed {frame_count} frames in {total_time:.2f} seconds.\\n\")\n",
    "    log_file.write(f\"FPS: {fps:.2f}\\n\")\n",
    "    log_file.close()\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if show_preview:\n",
    "        cv2.destroyAllWindows()\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Replace 'input_video.mp4' with the path to your video file or use \"camera\" for live feed\n",
    "    process_video('../ExampleVideos/surry.mp4', show_preview=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IA_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
